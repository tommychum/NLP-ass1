{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is ridiculous....'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 0][\"text\"].values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 1][\"text\"].values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2133b090>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x= 'target',data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the NAs we see that the \"location\" has around 33% of missing values in both datasets\n",
    "so in our opinion it is better to drop this column\n",
    "Moreover, the keyword column has less than 1% of missing values so we will keep it in case it will be useful in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset has a lot of unstructured tweets which should be \"cleaned\" in order to make an NLP model\n",
    "Removing punctuations, stop words will save more computational power  and give us a higher accuracy since they are not related to sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Removing HTTP links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2172\n",
       "0    1799\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many http words has this text?\n",
    "train.loc[train['text'].str.contains('http')].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column to show if the tweet has a link (maybe retweeted or a normal link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train['link'] = 0 \n",
    "train.link.loc[train['text'].str.contains('http')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi\n",
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "    \n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def remove_links(text):\n",
    "    no_link= pattern.sub('',text)\n",
    "    return no_link\n",
    "\n",
    "print(train['text'].iloc[33])\n",
    "print(remove_links(train['text'].iloc[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Removing duplicates but keeping count\n",
    "\n",
    "We realized we have a lot of retweets, so after removing the https: which means a retweet, we will remove all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CountofRetweets'] = 0\n",
    "train['CountofRetweets'] = train.groupby(['text']).transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset='text', keep=\"last\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Removing usernames (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ablaze what time does your talk go until? I don't know if I can make it due to work.\n",
      " what time does your talk go until? I don't know if I can make it due to work.\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('@[^\\s]+')\n",
    "\n",
    "def remove_username(text):\n",
    "    no_username= pattern.sub('',text)\n",
    "    return no_username\n",
    "\n",
    "print(train['text'].iloc[65])\n",
    "print(remove_links(train['text'].iloc[65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_username(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: remove_username(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Expanding shortened words (don't to do not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT REMOVE STOP WORDS, DONT IS IMPORTANT FOR NEGATION, filter?\n",
    "## WHAT TO DO WITH WEBSITES, create new column yes and no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "I am on top of the hill and I can see a fire in the woods...\n"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "print(train['text'].iloc[7])\n",
    "print(expand_contractions(train['text'].iloc[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Removal of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,000 people receive #wildfires evacuation orders in California \n",
      "13,000 people receive wildfires evacuation orders in California\n"
     ]
    }
   ],
   "source": [
    "# Our dataset is related to tweets so we will have a lot of @ and # \n",
    "from textblob import TextBlob\n",
    "\n",
    "def punctuations(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "print(train['text'].iloc[3])\n",
    "print(punctuations(expand_contractions(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Removal of accented characters (café to cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this dataset we do not have accented characters, this function will be used in case we are analyzing tweets \n",
    "# from France or any country that has accented characters in their languages\n",
    "import unidecode\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "# print(train['text'].iloc[3])\n",
    "# print((remove_accented_chars(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- Removal of repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(train)):\n",
    "    train.text.iloc[i] = re.sub(r'(.)\\1+', r'\\1\\1', train.text.iloc[i])\n",
    "\n",
    "\n",
    "for i in range(0,len(test)):\n",
    "    test.text.iloc[i] = re.sub(r'(.)\\1+', r'\\1\\1', test.text.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- Removal of commonly used words and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining flooding Florida TampaBay Tampa 18 or 19 days I have lost count\n",
      "['raining', 'flooding', 'florida', 'tampabay', 'tampa', 'days', 'lost', 'count']\n"
     ]
    }
   ],
   "source": [
    "#Option1 \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def common_stopwords(tweet):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "#     clean_mess = ' '.join(clean_mess)\n",
    "    return clean_mess\n",
    "\n",
    "print(train['text'].iloc[12])\n",
    "print(common_stopwords(train['text'].iloc[12]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option2 \n",
    "# We decided not to import the stopwords from nltk.corpus since we wanted to keep the words that negate like no,not,,\n",
    "\n",
    "# ### list of stop words that need to be removed\n",
    "# stop_words = ['as', 'in', 'of', 'is', 'are', 'were', 'was', 'it', 'for', 'to', 'from', 'into', 'onto', \n",
    "#               'this', 'that', 'being', 'the','those', 'these', 'such', 'a', 'an','i','and','be','you',\n",
    "#               'have','on','my','do','with', 'or','be','at','by','s','have']\n",
    "\n",
    "# from nltk import word_tokenize\n",
    "# import re\n",
    "\n",
    "# def remove_stopwords(tweet):\n",
    "#     tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     tokenized_words = word_tokenize(tweet)\n",
    "#     temp = [word for word in tokenized_words if word not in stop_words]\n",
    "# #     temp = ' '.join(temp)\n",
    "#     return temp\n",
    "\n",
    "# print(train['text'].iloc[12])\n",
    "# print(remove_stopwords(train['text'].iloc[12]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between option A and B, we got a lower score using option B where we filter manual the stop words, so we will use option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(common_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(common_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raining', 'flooding', 'florida', 'tampabay', 'tampa', 'days', 'lost', 'count']\n",
      "['rain', 'flood', 'florida', 'tampabay', 'tampa', 'days', 'lose', 'count']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "print((train['text'].iloc[12]))   \n",
    "print(normalization((train['text'].iloc[12])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Creating word count variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WordCount'] = train['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "      <th>CountofRetweets</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>norm_count_word</th>\n",
       "      <th>norm_count_retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents ask shelter place notify officer eva...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation order cali...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get send photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6984</td>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridge collapse nearby home</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6985</td>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6986</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>utc km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6987</td>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigate e bike collide car little p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6988</td>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfire ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6989 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                                               text  \\\n",
       "0         1     NaN       deeds reason earthquake may allah forgive us   \n",
       "1         4     NaN              forest fire near la ronge sask canada   \n",
       "2         5     NaN  residents ask shelter place notify officer eva...   \n",
       "3         6     NaN  people receive wildfires evacuation order cali...   \n",
       "4         7     NaN  get send photo ruby alaska smoke wildfires pou...   \n",
       "...     ...     ...                                                ...   \n",
       "6984  10869     NaN   two giant crane hold bridge collapse nearby home   \n",
       "6985  10870     NaN  control wild fire california even northern par...   \n",
       "6986  10871     NaN                              utc km volcano hawaii   \n",
       "6987  10872     NaN  police investigate e bike collide car little p...   \n",
       "6988  10873     NaN  latest home raze northern california wildfire ...   \n",
       "\n",
       "      target  link  CountofRetweets  WordCount  norm_count_word  \\\n",
       "0          1     0                1          7              NaN   \n",
       "1          1     0                1          7              NaN   \n",
       "2          1     0                1         11              NaN   \n",
       "3          1     0                1          6              NaN   \n",
       "4          1     0                1          9              NaN   \n",
       "...      ...   ...              ...        ...              ...   \n",
       "6984       1     1                8          8              NaN   \n",
       "6985       1     0                2          9              NaN   \n",
       "6986       1     1                2          4              NaN   \n",
       "6987       1     0                2         17              NaN   \n",
       "6988       1     1               13          8              NaN   \n",
       "\n",
       "      norm_count_retweets  \n",
       "0                0.000000  \n",
       "1                0.000000  \n",
       "2                0.000000  \n",
       "3                0.000000  \n",
       "4                0.000000  \n",
       "...                   ...  \n",
       "6984             0.304348  \n",
       "6985             0.043478  \n",
       "6986             0.043478  \n",
       "6987             0.043478  \n",
       "6988             0.521739  \n",
       "\n",
       "[6989 rows x 9 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['WordCount'] = train['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train)):\n",
    "        train.text.iloc[i] = ' '.join(train.text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       residents ask shelter place notify officer eva...\n",
       "3       people receive wildfires evacuation order cali...\n",
       "4       get send photo ruby alaska smoke wildfires pou...\n",
       "                              ...                        \n",
       "6984     two giant crane hold bridge collapse nearby home\n",
       "6985    control wild fire california even northern par...\n",
       "6986                                utc km volcano hawaii\n",
       "6987    police investigate e bike collide car little p...\n",
       "6988    latest home raze northern california wildfire ...\n",
       "Name: text, Length: 6989, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Frequent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>get</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>like</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fire</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>amp</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>go</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bomb</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>one</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>say</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>people</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>would</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Frequent\n",
       "get               418\n",
       "like              383\n",
       "fire              339\n",
       "amp               333\n",
       "go                309\n",
       "bomb              201\n",
       "one               195\n",
       "say               189\n",
       "people            185\n",
       "would             185"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create our dictionary \n",
    "uniqueWordFrequents = {}\n",
    "for tweet in train.text:\n",
    "    for word in tweet.split():\n",
    "        if(word in uniqueWordFrequents.keys()):\n",
    "            uniqueWordFrequents[word] += 1\n",
    "        else:\n",
    "            uniqueWordFrequents[word] = 1\n",
    "            \n",
    "#Convert dictionary to dataFrame\n",
    "uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
    "uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
    "uniqueWordFrequents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 1)\n"
     ]
    }
   ],
   "source": [
    "uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
    "print(uniqueWordFrequents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the features that were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.272727\n",
       "1       0.272727\n",
       "2       0.454545\n",
       "3       0.227273\n",
       "4       0.363636\n",
       "          ...   \n",
       "6984    0.318182\n",
       "6985    0.363636\n",
       "6986    0.136364\n",
       "6987    0.727273\n",
       "6988    0.318182\n",
       "Name: norm_count_word, Length: 6989, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_count_word']=(train.WordCount-train.WordCount.min())/(train.WordCount.max()-train.WordCount.min())\n",
    "train.norm_count_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       0.000000\n",
       "3       0.000000\n",
       "4       0.000000\n",
       "          ...   \n",
       "6984    0.304348\n",
       "6985    0.043478\n",
       "6986    0.043478\n",
       "6987    0.043478\n",
       "6988    0.521739\n",
       "Name: norm_count_retweets, Length: 6989, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_count_retweets']=(train.CountofRetweets-train.CountofRetweets.min())/(train.CountofRetweets.max()-train.CountofRetweets.min())\n",
    "train.norm_count_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
    "# bagOfWords = counVec.fit_transform(train.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=uniqueWordFrequents.shape[0], min_df=5, max_df=0.7)  \n",
    "X1 = tfidfconverter.fit_transform(train.text).toarray()\n",
    "#if remove X, bagofwords should be X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding normalized variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       0.000000\n",
       "3       0.000000\n",
       "4       0.000000\n",
       "          ...   \n",
       "6984    0.304348\n",
       "6985    0.043478\n",
       "6986    0.043478\n",
       "6987    0.043478\n",
       "6988    0.521739\n",
       "Name: norm_count_retweets, Length: 6989, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.norm_count_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.DataFrame(X1)\n",
    "\n",
    "new_X = pd.concat([X, train.norm_count_retweets], axis = 1,ignore_index = True)\n",
    "new_X = pd.concat([new_X, train.norm_count_word], axis = 1,ignore_index = True)\n",
    "new_X = pd.concat([new_X, train.link], axis = 1, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6989 rows × 657 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  647  648  649  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "6984  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6985  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6986  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6987  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "6988  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      650  651  652  653       654       655  656  \n",
       "0     0.0  0.0  0.0  0.0  0.000000  0.272727    0  \n",
       "1     0.0  0.0  0.0  0.0  0.000000  0.272727    0  \n",
       "2     0.0  0.0  0.0  0.0  0.000000  0.454545    0  \n",
       "3     0.0  0.0  0.0  0.0  0.000000  0.227273    0  \n",
       "4     0.0  0.0  0.0  0.0  0.000000  0.363636    0  \n",
       "...   ...  ...  ...  ...       ...       ...  ...  \n",
       "6984  0.0  0.0  0.0  0.0  0.304348  0.318182    1  \n",
       "6985  0.0  0.0  0.0  0.0  0.043478  0.363636    0  \n",
       "6986  0.0  0.0  0.0  0.0  0.043478  0.136364    1  \n",
       "6987  0.0  0.0  0.0  0.0  0.043478  0.727273    0  \n",
       "6988  0.0  0.0  0.0  0.0  0.521739  0.318182    1  \n",
       "\n",
       "[6989 rows x 657 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# text_clf = Pipeline([('vect', CountVectorizer(max_features = uniqueWordFrequents.shape[0], min_df=5, max_df=0.7 )),\n",
    "#                       ('tfidf', TfidfTransformer()),\n",
    "#                       ('clf', MultinomialNB()) ])\n",
    "# text_clf = text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = text_clf.predict(X_test)\n",
    "# print(' F1 Score is      : ' ,f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (6989, 654)\n",
      "y shape =  (6989,)\n",
      "data splitting successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['target']\n",
    "print(\"X shape = \",X.shape)\n",
    "print(\"y shape = \",y.shape)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(new_X,y,test_size=0.20, random_state=55, shuffle =True)\n",
    "print('data splitting successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision Tree Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = None, \n",
    "                                           splitter='best', \n",
    "                                           random_state=55)\n",
    "\n",
    "decisionTreeModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"decision Tree Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression = LogisticRegression(penalty='l2', \n",
    "                                        solver='saga', \n",
    "                                        random_state = 55)  \n",
    "\n",
    "LogisticRegression.fit(X_train,y_train)\n",
    "\n",
    "print(\"LogisticRegression Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVClassifier model run successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVClassifier = SVC(kernel= 'linear',\n",
    "                   degree=3,\n",
    "                   max_iter=10000,\n",
    "                   C=2, \n",
    "                   random_state = 55)\n",
    "\n",
    "SVClassifier.fit(X_train,y_train)\n",
    "\n",
    "print(\"SVClassifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient Boosting Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 100,\n",
    "                                                   max_depth = 30,\n",
    "                                                   random_state=55)\n",
    "\n",
    "gradientBoostingModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"gradient Boosting Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomialNBModel = MultinomialNB(alpha=0.1)\n",
    "multinomialNBModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"multinomialNB model run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier  Train Score is   :  0.9896261849400823\n",
      "DecisionTreeClassifier  Test Score is    :  0.7124463519313304\n",
      "DecisionTreeClassifier  F1 Score is      :  0.6242990654205608\n",
      "--------------------------------------------------------------------------\n",
      "GradientBoostingClassifier  Train Score is   :  0.8715793239134323\n",
      "GradientBoostingClassifier  Test Score is    :  0.765379113018598\n",
      "GradientBoostingClassifier  F1 Score is      :  0.6339285714285714\n",
      "--------------------------------------------------------------------------\n",
      "LogisticRegression  Train Score is   :  0.8193525308531568\n",
      "LogisticRegression  Test Score is    :  0.7889842632331903\n",
      "LogisticRegression  F1 Score is      :  0.704112337011033\n",
      "--------------------------------------------------------------------------\n",
      "SVC  Train Score is   :  0.8300840636737614\n",
      "SVC  Test Score is    :  0.7882689556509299\n",
      "SVC  F1 Score is      :  0.705765407554672\n",
      "--------------------------------------------------------------------------\n",
      "MultinomialNB  Train Score is   :  0.7948488642461098\n",
      "MultinomialNB  Test Score is    :  0.7796852646638054\n",
      "MultinomialNB  F1 Score is      :  0.6962524654832347\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#evaluation Details\n",
    "models = [decisionTreeModel, gradientBoostingModel,  LogisticRegression,\n",
    "          SVClassifier, multinomialNBModel]\n",
    "\n",
    "for model in models:\n",
    "    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n",
    "    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "      <th>CountofRetweets</th>\n",
       "      <th>WordCount</th>\n",
       "      <th>norm_count_word</th>\n",
       "      <th>norm_count_retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents ask shelter place notify officer eva...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation order cali...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get send photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6984</td>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant crane hold bridge collapse nearby home</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6985</td>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6986</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>utc km volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6987</td>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigate e bike collide car little p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6988</td>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest home raze northern california wildfire ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6989 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                                               text  \\\n",
       "0         1     NaN       deeds reason earthquake may allah forgive us   \n",
       "1         4     NaN              forest fire near la ronge sask canada   \n",
       "2         5     NaN  residents ask shelter place notify officer eva...   \n",
       "3         6     NaN  people receive wildfires evacuation order cali...   \n",
       "4         7     NaN  get send photo ruby alaska smoke wildfires pou...   \n",
       "...     ...     ...                                                ...   \n",
       "6984  10869     NaN   two giant crane hold bridge collapse nearby home   \n",
       "6985  10870     NaN  control wild fire california even northern par...   \n",
       "6986  10871     NaN                              utc km volcano hawaii   \n",
       "6987  10872     NaN  police investigate e bike collide car little p...   \n",
       "6988  10873     NaN  latest home raze northern california wildfire ...   \n",
       "\n",
       "      target  link  CountofRetweets  WordCount  norm_count_word  \\\n",
       "0          1     0                1          7         0.272727   \n",
       "1          1     0                1          7         0.272727   \n",
       "2          1     0                1         11         0.454545   \n",
       "3          1     0                1          6         0.227273   \n",
       "4          1     0                1          9         0.363636   \n",
       "...      ...   ...              ...        ...              ...   \n",
       "6984       1     1                8          8         0.318182   \n",
       "6985       1     0                2          9         0.363636   \n",
       "6986       1     1                2          4         0.136364   \n",
       "6987       1     0                2         17         0.727273   \n",
       "6988       1     1               13          8         0.318182   \n",
       "\n",
       "      norm_count_retweets  \n",
       "0                0.000000  \n",
       "1                0.000000  \n",
       "2                0.000000  \n",
       "3                0.000000  \n",
       "4                0.000000  \n",
       "...                   ...  \n",
       "6984             0.304348  \n",
       "6985             0.043478  \n",
       "6986             0.043478  \n",
       "6987             0.043478  \n",
       "6988             0.521739  \n",
       "\n",
       "[6989 rows x 9 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "#     ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on accuracy choose option 1 or 2 in stop words and try to  remove not repetetive words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
