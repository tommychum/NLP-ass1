{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is ridiculous....'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 0][\"text\"].values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 1][\"text\"].values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a166817d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x= 'target',data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the NAs we see that the \"location\" has around 33% of missing values in both datasets\n",
    "so in our opinion it is better to drop this column\n",
    "Moreover, the keyword column has less than 1% of missing values so we will keep it in case it will be useful in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation_count\n",
    "import string\n",
    "train['punctuation_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# stop_word_count\n",
    "from wordcloud import STOPWORDS\n",
    "train['stop_word_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "test['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# char_count\n",
    "train['char_count'] = train['text'].apply(lambda x: len(str(x)))\n",
    "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# word_count\n",
    "train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "train['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "\n",
    "# mean_word_length\n",
    "import numpy as np\n",
    "train['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": []
=======
   "source": [
    "#### Creating new variables"
   ]
>>>>>>> parent of 59b15c8... Update Twitter_CYRILGEBARA+TOMMASOMERCALDO-Copy2.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation_count\n",
    "import string\n",
    "train['punctuation_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# stop_word_count\n",
    "from wordcloud import STOPWORDS\n",
    "train['stop_word_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "test['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "# char_count\n",
    "train['char_count'] = train['text'].apply(lambda x: len(str(x)))\n",
    "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# word_count\n",
    "train['word_count'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# unique_word_count\n",
    "train['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "\n",
    "# mean_word_length\n",
    "import numpy as np\n",
    "train['mean_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset has a lot of unstructured tweets which should be \"cleaned\" in order to make an NLP model\n",
    "Removing punctuations, stop words will save more computational power  and give us a higher accuracy since they are not related to sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Removing Duplicates/Fixing the mislabled targets\n",
    "\n",
    "We realized we have a lot of retweets, so after removing the https: which means a retweet. We tried removing the duplicates but we got a score lower by 0.02, so we will just fix the mislabled targets by creating a dataframe that has the mislabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n",
       " 'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!',\n",
       " \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
       " 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE',\n",
       " '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption',\n",
       " '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect',\n",
       " 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam',\n",
       " 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG',\n",
       " 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife',\n",
       " 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring',\n",
       " \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\n",
       " 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon',\n",
       " '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4',\n",
       " 'Caution: breathing may be hazardous to your health.',\n",
       " 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????',\n",
       " 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
    "df_mislabeled.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refixing the target variable of the 18 mislabled texts \n",
    "train['target_relabeled'] = train['target'].copy() \n",
    "\n",
    "train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and \\\n",
    "                                police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control\\\n",
    "                                you! #Afterlife', 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally \\\n",
    "                                displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver \\\n",
    "                                Spring', 'target_relabeled'] = 1\n",
    "train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe\\\n",
    "                                and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the \\\n",
    "                                opposite of terrorism!', 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God\\\n",
    "                                is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper.\\\n",
    "                                http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "train.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building....\\\n",
    "                                    Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] \\\n",
    "                                    = 0\n",
    "train.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land\\\n",
    "                              of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do \\\n",
    "                             anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is \\\n",
    "                                                    by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "train.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\",\\\n",
    "                        'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description\\\n",
    "                            of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "train.loc[train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while\\\n",
    "                            and you realise its been on 3G this whole time\", 'target_relabeled'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Removing HTTP links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2172\n",
       "0    1799\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many http words has this text?\n",
    "train.loc[train['text'].str.contains('http')].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi\n",
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "    \n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "def remove_links(text):\n",
    "    no_link= pattern.sub('',text)\n",
    "    return no_link\n",
    "\n",
    "print(\"Example: \")\n",
    "print(train['text'].iloc[33])\n",
    "print(remove_links(train['text'].iloc[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_links(x))\n",
    "test['text'] = test['text'].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Removing usernames (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "@nxwestmidlands huge fire at Wholesale markets ablaze \n",
      " huge fire at Wholesale markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('@[^\\s]+')\n",
    "\n",
    "def remove_username(text):\n",
    "    no_username= pattern.sub('',text)\n",
    "    return no_username\n",
    "print(\"Example: \")\n",
    "print(train['text'].iloc[65])\n",
    "print(remove_links(train['text'].iloc[65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_username(x))\n",
    "test['text'] = test['text'].apply(lambda x: remove_username(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Expanding shortened words (don't to do not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "I am on top of the hill and I can see a fire in the woods...\n"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "print(\"Example: \")\n",
    "print(train['text'].iloc[7])\n",
    "print(expand_contractions(train['text'].iloc[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(expand_contractions)\n",
    "test['text'] = test['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Removal of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "13,000 people receive #wildfires evacuation orders in California \n",
      "13,000 people receive wildfires evacuation orders in California\n"
     ]
    }
   ],
   "source": [
    "# Our dataset is related to tweets so we will have a lot of @ and # \n",
    "from textblob import TextBlob\n",
    "\n",
    "def punctuations(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "print(\"Example: \")\n",
    "print(train['text'].iloc[3])\n",
    "print(punctuations(expand_contractions(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(punctuations)\n",
    "test['text'] = test['text'].apply(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- Removal of accented characters (café to cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this dataset we do not have accented characters, this function will be used in case we are analyzing tweets \n",
    "# from France or any country that has accented characters in their languages\n",
    "import unidecode\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "# print(train['text'].iloc[3])\n",
    "# print((remove_accented_chars(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(remove_accented_chars)\n",
    "test['text'] = test['text'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- Removal of repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      "Cooool\n",
      "Cool\n"
     ]
    }
   ],
   "source": [
    "def remove_repeated(txt):\n",
    "    cleaned = re.sub(r'(.)\\1+', r'\\1\\1', txt)\n",
    "    return cleaned\n",
    "\n",
    "print(\"Example: \")\n",
    "print(train['text'].iloc[28])\n",
    "print((remove_repeated(train['text'].iloc[28])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(remove_repeated)\n",
    "test['text'] = test['text'].apply(remove_repeated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Removal of Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(txt):\n",
    "    emoji_pattern = re.compile(\"[\" u\"\\U000024C2-\\U0001F251\"\n",
    "                                       u\"\\U00002702-\\U000027B0\"\n",
    "                                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                                                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                                                                               \"]+\", flags = re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', txt)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_emoji)\n",
    "test['text'] = test['text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Removal of commonly used words and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option1 \n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# def common_stopwords(text):\n",
    "#     tokenized_words = word_tokenize(text)\n",
    "#     temp = [word for word in tokenized_words if word not in STOPWORDS]\n",
    "#     temp = ' '.join(temp)\n",
    "#     return temp\n",
    "\n",
    "# print(train['text'].iloc[12])\n",
    "# print(common_stopwords(train['text'].iloc[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining flooding Florida TampaBay Tampa 18 or 19 days I have lost count\n",
      "['raining', 'flooding', 'florida', 'tampabay', 'tampa', 'or', 'days', 'i', 'have', 'lost', 'count']\n"
     ]
    }
   ],
   "source": [
    "#Option1 \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def common_stopwords(tweet):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "    clean_mess = ' '.join(clean_mess)\n",
    "    return clean_tokens\n",
    "\n",
    "print(train['text'].iloc[12])\n",
    "print(common_stopwords(train['text'].iloc[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option2 \n",
    "# We decided not to import the stopwords from nltk.corpus since we wanted to keep the words that negate like no,not,,\n",
    "\n",
    "# ### list of stop words that need to be removed\n",
    "# stop_words = ['as', 'in', 'of', 'is', 'are', 'were', 'was', 'it', 'for', 'to', 'from', 'into', 'onto', \n",
    "#               'this', 'that', 'being', 'the','those', 'these', 'such', 'a', 'an','i','and','be','you',\n",
    "#               'have','on','my','do','with', 'or','be','at','by','s','have']\n",
    "\n",
    "# from nltk import word_tokenize\n",
    "# import re\n",
    "\n",
    "# def remove_stopwords(tweet):\n",
    "#     tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     tokenized_words = word_tokenize(tweet)\n",
    "#     temp = [word for word in tokenized_words if word not in stop_words]\n",
    "# #     temp = ' '.join(temp)\n",
    "#     return temp\n",
    "\n",
    "# print(train['text'].iloc[12])\n",
    "# print(remove_stopwords(train['text'].iloc[12]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between option A and B, we got a lower score using option B where we filter manually the stop words, so we will use option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(common_stopwords)\n",
    "test['text'] = test['text'].apply(common_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raining', 'flooding', 'florida', 'tampabay', 'tampa', 'or', 'days', 'i', 'have', 'lost', 'count']\n",
      "['rain', 'flood', 'florida', 'tampabay', 'tampa', 'or', 'days', 'i', 'have', 'lose', 'count']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "print((train['text'].iloc[12]))   \n",
    "print(normalization((train['text'].iloc[12])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(normalization)\n",
    "test['text'] = test['text'].apply(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds be the reason of this earthquake may...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents ask to shelter in place be be no...\n",
       "3       people receive wildfires evacuation order in c...\n",
       "4       just get send this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant crane hold a bridge collapse into ne...\n",
       "7609    the out of control wild fire in california eve...\n",
       "7610                         m utc km s of volcano hawaii\n",
       "7611    police investigate after an e bike collide wit...\n",
       "7612    the latest more home raze by northern californ...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After preprocessing, the text format\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x : combine_text(x))\n",
    "test['text'] = test['text'].apply(lambda x : combine_text(x))\n",
    "train['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create our dictionary \n",
    "# uniqueWordFrequents = {}\n",
    "# for tweet in train.text:\n",
    "#     for word in tweet.split():\n",
    "#         if(word in uniqueWordFrequents.keys()):\n",
    "#             uniqueWordFrequents[word] += 1\n",
    "#         else:\n",
    "#             uniqueWordFrequents[word] = 1\n",
    "            \n",
    "# #Convert dictionary to dataFrame\n",
    "# uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
    "# uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
    "# uniqueWordFrequents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
    "# print(uniqueWordFrequents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the features that were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.400000\n",
       "1       0.200000\n",
       "2       0.700000\n",
       "3       0.233333\n",
       "4       0.500000\n",
       "          ...   \n",
       "7608    0.333333\n",
       "7609    0.633333\n",
       "7610    0.233333\n",
       "7611    0.600000\n",
       "7612    0.400000\n",
       "Name: norm_word_count, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_word_count']=(train.word_count-train.word_count.min())/(train.word_count.max()-train.word_count.min())\n",
    "train.norm_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.428571\n",
       "1       0.214286\n",
       "2       0.678571\n",
       "3       0.250000\n",
       "4       0.500000\n",
       "          ...   \n",
       "7608    0.357143\n",
       "7609    0.571429\n",
       "7610    0.250000\n",
       "7611    0.642857\n",
       "7612    0.428571\n",
       "Name: norm_unique_word_count, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_unique_word_count']=(train.unique_word_count-train.unique_word_count.min())/(train.unique_word_count.max()-train.unique_word_count.min())\n",
    "train.norm_unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.016393\n",
       "1       0.016393\n",
       "2       0.049180\n",
       "3       0.032787\n",
       "4       0.032787\n",
       "          ...   \n",
       "7608    0.081967\n",
       "7609    0.081967\n",
       "7610    0.180328\n",
       "7611    0.081967\n",
       "7612    0.114754\n",
       "Name: norm_punctuation_count, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_punctuation_count']=(train.punctuation_count-train.punctuation_count.min())/(train.punctuation_count.max()-train.punctuation_count.min())\n",
    "train.norm_punctuation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.315789\n",
       "1       0.000000\n",
       "2       0.578947\n",
       "3       0.052632\n",
       "4       0.368421\n",
       "          ...   \n",
       "7608    0.105263\n",
       "7609    0.473684\n",
       "7610    0.052632\n",
       "7611    0.263158\n",
       "7612    0.157895\n",
       "Name: norm_stop_word_count, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_stop_word_count']=(train.stop_word_count-train.stop_word_count.min())/(train.stop_word_count.max()-train.stop_word_count.min())\n",
    "train.norm_stop_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.413333\n",
       "1       0.206667\n",
       "2       0.840000\n",
       "3       0.386667\n",
       "4       0.540000\n",
       "          ...   \n",
       "7608    0.506667\n",
       "7609    0.786667\n",
       "7610    0.386667\n",
       "7611    0.866667\n",
       "7612    0.580000\n",
       "Name: norm_char_count, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_char_count']=(train.char_count-train.char_count.min())/(train.char_count.max()-train.char_count.min())\n",
    "train.norm_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.124953\n",
       "1       0.135889\n",
       "2       0.166297\n",
       "3       0.285366\n",
       "4       0.131707\n",
       "          ...   \n",
       "7608    0.256763\n",
       "7609    0.178537\n",
       "7610    0.292683\n",
       "7611    0.234917\n",
       "7612    0.237523\n",
       "Name: norm_mean_word_length, Length: 7613, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['norm_mean_word_length']=(train.mean_word_length-train.mean_word_length.min())/(train.mean_word_length.max()-train.mean_word_length.min())\n",
    "train.norm_mean_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We got a higher score when using tfidf\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
    "# bagOfWords = counVec.fit_transform(train.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# if we used the uniqueWordFrequents, but we got a lower score with it\n",
    "\n",
    "# tfidfconverter = TfidfVectorizer(max_features=uniqueWordFrequents.shape[0], min_df=5, max_df=0.7)  \n",
    "tfidfconverter = TfidfVectorizer( min_df=5, max_df=0.7)  \n",
    "X1 = tfidfconverter.fit_transform(train.text).toarray()\n",
    "#if remove X, bagofwords should be X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (7613, 2438)\n",
      "y shape =  (7613,)\n",
      "data splitting successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['target']\n",
    "print(\"X shape = \",X1.shape)\n",
    "print(\"y shape = \",y.shape)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X1,y,test_size=0.20, random_state=55, shuffle =True)\n",
    "print('data splitting successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision Tree Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = None, \n",
    "                                           splitter='best', \n",
    "                                           random_state=55)\n",
    "\n",
    "decisionTreeModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"decision Tree Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression = LogisticRegression(penalty='l2', \n",
    "                                        solver='saga', \n",
    "                                        random_state = 55)  \n",
    "\n",
    "LogisticRegression.fit(X_train,y_train)\n",
    "\n",
    "print(\"LogisticRegression Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVClassifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVClassifier = SVC()\n",
    "\n",
    "SVClassifier.fit(X_train,y_train)\n",
    "\n",
    "print(\"SVClassifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient Boosting Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradientBoostingModel = GradientBoostingClassifier()\n",
    "\n",
    "gradientBoostingModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"gradient Boosting Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialNB model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomialNBModel = MultinomialNB(alpha=0.1)\n",
    "multinomialNBModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"multinomialNB model run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier  Train Score is   :  0.983743842364532\n",
      "DecisionTreeClassifier  Test Score is    :  0.721602101116218\n",
      "DecisionTreeClassifier  F1 Score is      :  0.665615141955836\n",
      "--------------------------------------------------------------------------\n",
      "GradientBoostingClassifier  Train Score is   :  0.7940886699507389\n",
      "GradientBoostingClassifier  Test Score is    :  0.7583716349310571\n",
      "GradientBoostingClassifier  F1 Score is      :  0.6592592592592592\n",
      "--------------------------------------------------------------------------\n",
      "LogisticRegression  Train Score is   :  0.8599343185550082\n",
      "LogisticRegression  Test Score is    :  0.7971109652002626\n",
      "LogisticRegression  F1 Score is      :  0.7392405063291139\n",
      "--------------------------------------------------------------------------\n",
      "SVC  Train Score is   :  0.9446633825944171\n",
      "SVC  Test Score is    :  0.8069599474720945\n",
      "SVC  F1 Score is      :  0.7411971830985915\n",
      "--------------------------------------------------------------------------\n",
      "MultinomialNB  Train Score is   :  0.8528735632183908\n",
      "MultinomialNB  Test Score is    :  0.7859487852921865\n",
      "MultinomialNB  F1 Score is      :  0.7179930795847751\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#evaluation Details\n",
    "models = [decisionTreeModel, gradientBoostingModel,  LogisticRegression,\n",
    "          SVClassifier, multinomialNBModel]\n",
    "\n",
    "for model in models:\n",
    "    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n",
    "    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the new features we got a similar accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.DataFrame(X1)\n",
    "train1 = pd.concat([X1, train.norm_word_count], axis = 1)\n",
    "train1 = pd.concat([train1, train.norm_unique_word_count], axis = 1)\n",
    "train1 = pd.concat([train1, train.norm_punctuation_count], axis = 1)\n",
    "train1 = pd.concat([train1, train.norm_stop_word_count], axis = 1)\n",
    "train1 = pd.concat([train1, train.norm_char_count], axis = 1)\n",
    "train1 = pd.concat([train1, train.norm_mean_word_length], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape =  (7613, 2444)\n",
      "y shape =  (7613,)\n",
      "data splitting successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['target']\n",
    "print(\"X shape = \",train1.shape)\n",
    "print(\"y shape = \",y.shape)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(train1,y,test_size=0.20, random_state=55, shuffle =True)\n",
    "print('data splitting successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Classifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression1 = LogisticRegression(penalty='l2', \n",
    "                                        solver='saga', \n",
    "                                        random_state = 55)  \n",
    "\n",
    "LogisticRegression1.fit(X_train,y_train)\n",
    "\n",
    "print(\"LogisticRegression Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVClassifier model run successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVClassifier1 = SVC()\n",
    "\n",
    "SVClassifier1.fit(X_train,y_train)\n",
    "\n",
    "print(\"SVClassifier model run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  Train Score is   :  0.8614121510673235\n",
      "LogisticRegression  Test Score is    :  0.7925147734734077\n",
      "LogisticRegression  F1 Score is      :  0.7335581787521079\n",
      "--------------------------------------------------------------------------\n",
      "SVC  Train Score is   :  0.9036124794745485\n",
      "SVC  Test Score is    :  0.8036769533814839\n",
      "SVC  F1 Score is      :  0.7406764960971378\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#evaluation Details\n",
    "models = [LogisticRegression1,\n",
    "          SVClassifier1]\n",
    "\n",
    "for model in models:\n",
    "    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n",
    "    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = tfidfconverter.fit_transform(test.text).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-7b5ed684e99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=np.float64, order=\"C\",\n\u001b[0;32m--> 447\u001b[0;31m                         accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm'"
     ]
    }
   ],
   "source": [
    "sample_submission[\"target\"] = SVClassifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
