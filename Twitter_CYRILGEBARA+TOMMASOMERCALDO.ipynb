{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is ridiculous....'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 0][\"text\"].values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 1][\"text\"].values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a16ded690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x= 'target',data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the NAs we see that the \"location\" has around 33% of missing values in both datasets\n",
    "so in our opinion it is better to drop this column\n",
    "Moreover, the keyword column has less than 1% of missing values so we will keep it in case it will be useful in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['location']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset has a lot of unstructured tweets which should be \"cleaned\" in order to make an NLP model\n",
    "Removing punctuations, stop words will save more computational power  and give us a higher accuracy since they are not related to sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Removing HTTP links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2172\n",
       "0    1799\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many http words has this text?\n",
    "train.loc[train['text'].str.contains('http')].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column to show if the tweet has a link (maybe retweeted or a normal link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train['link'] = 0 \n",
    "train.link.loc[train['text'].str.contains('http')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi\n",
      "#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "    \n",
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def remove_links(text):\n",
    "    no_link= pattern.sub('',text)\n",
    "    return no_link\n",
    "\n",
    "print(train['text'].iloc[33])\n",
    "print(remove_links(train['text'].iloc[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: remove_links(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- We realized we have a lot of retweets, so after removing the https: which means a retweet, we will remove all duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>CountofRetweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\\nANOTHER DISASTER WAITING TO HAPPEN AND YOUR ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\nCROYDON RIOTS- The Next Day: Burning Buildin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\nSON OF SAVIOR LAVA VIDEO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\nSeems they declared war against government..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Cindy Noonan@CindyNoonan-Heartbreak in #Balt...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6984</td>\n",
       "      <td>Û÷We Can HelpÛª Says Denver Firefighter Work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6985</td>\n",
       "      <td>å_? New Ladies Shoulder Tote #Handbag Faux Lea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6986</td>\n",
       "      <td>å¤} New Ladies Shoulder Tote #Handbag Faux Lea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6987</td>\n",
       "      <td>å¬'Only the sea knows how many are dead' @MSF_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6988</td>\n",
       "      <td>åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  CountofRetweets\n",
       "0     \\nANOTHER DISASTER WAITING TO HAPPEN AND YOUR ...                1\n",
       "1     \\nCROYDON RIOTS- The Next Day: Burning Buildin...                1\n",
       "2                            \\nSON OF SAVIOR LAVA VIDEO                1\n",
       "3        \\nSeems they declared war against government..                1\n",
       "4       Cindy Noonan@CindyNoonan-Heartbreak in #Balt...                2\n",
       "...                                                 ...              ...\n",
       "6984  Û÷We Can HelpÛª Says Denver Firefighter Work...                1\n",
       "6985  å_? New Ladies Shoulder Tote #Handbag Faux Lea...                1\n",
       "6986  å¤} New Ladies Shoulder Tote #Handbag Faux Lea...                1\n",
       "6987  å¬'Only the sea knows how many are dead' @MSF_...                1\n",
       "6988  åÈMGN-AFRICAå¨ pin:263789F4 åÈ Correction: Ten...                1\n",
       "\n",
       "[6989 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['text']).size().reset_index(name='CountofRetweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7602</td>\n",
       "      <td>10860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a siren just went off and it wasn't the Forney...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7603</td>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Officials say a quarantine is in place at an A...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7604</td>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: U...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7605</td>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the flip side I'm at Walmart and there is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7606</td>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security site...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6989 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                                               text  \\\n",
       "0         1     NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1         4     NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2         5     NaN  All residents asked to 'shelter in place' are ...   \n",
       "3         6     NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4         7     NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...     ...     ...                                                ...   \n",
       "7602  10860     NaN  a siren just went off and it wasn't the Forney...   \n",
       "7603  10862     NaN  Officials say a quarantine is in place at an A...   \n",
       "7604  10863     NaN  #WorldNews Fallen powerlines on G:link tram: U...   \n",
       "7605  10864     NaN  on the flip side I'm at Walmart and there is a...   \n",
       "7606  10866     NaN  Suicide bomber kills 15 in Saudi security site...   \n",
       "\n",
       "      target  link  \n",
       "0          1     0  \n",
       "1          1     0  \n",
       "2          1     0  \n",
       "3          1     0  \n",
       "4          1     0  \n",
       "...      ...   ...  \n",
       "7602       1     0  \n",
       "7603       1     1  \n",
       "7604       1     1  \n",
       "7605       1     0  \n",
       "7606       1     1  \n",
       "\n",
       "[6989 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop_duplicates(subset='text', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Removing usernames (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@nxwestmidlands huge fire at Wholesale markets ablaze \n",
      " huge fire at Wholesale markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('@[^\\s]+')\n",
    "\n",
    "def remove_username(text):\n",
    "    no_username= pattern.sub('',text)\n",
    "    return no_username\n",
    "\n",
    "print(train['text'].iloc[65])\n",
    "print(remove_links(train['text'].iloc[65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: remove_username(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: remove_username(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Expanding shortened words (don't to do not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT REMOVE STOP WORDS, DONT IS IMPORTANT FOR NEGATION, filter?\n",
    "## WHAT TO DO WITH WEBSITES, create new column yes and no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm on top of the hill and I can see a fire in the woods...\n",
      "I am on top of the hill and I can see a fire in the woods...\n"
     ]
    }
   ],
   "source": [
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "print(train['text'].iloc[7])\n",
    "print(expand_contractions(train['text'].iloc[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Removal of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,000 people receive #wildfires evacuation orders in California \n",
      "13,000 people receive wildfires evacuation orders in California\n"
     ]
    }
   ],
   "source": [
    "# Our dataset is related to tweets so we will have a lot of @ and # \n",
    "from textblob import TextBlob\n",
    "\n",
    "def punctuations(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "print(train['text'].iloc[3])\n",
    "print(punctuations(expand_contractions(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Removal of accented characters (café to cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this dataset we do not have accented characters, this function will be used in case we are analyzing tweets \n",
    "# from France or any country that has accented characters in their languages\n",
    "import unidecode\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "# print(train['text'].iloc[3])\n",
    "# print((remove_accented_chars(train['text'].iloc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(remove_accented_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All residents asked to 'shelter in place are being notified by officers No other evacuation or shelter in place orders are expected\n",
      "Al Residents Asked To 'Shelter In Place Are Being Notified By Oficers No Other Evacuation Or Shelter In Place Orders Are Expected\n"
     ]
    }
   ],
   "source": [
    "def repeatedletters(string):\n",
    "    postCheck = \"\"\n",
    "    previousChar = None\n",
    "    for currentChar in string:\n",
    "        if currentChar != previousChar:\n",
    "            postCheck += currentChar\n",
    "            previousChar = currentChar\n",
    "    return postCheck.title()\n",
    "\n",
    "print((train['text'].iloc[2]))   \n",
    "print(repeatedletters((train['text'].iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(repeatedletters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Removal of commonly used words and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raining Floding Florida Tampabay Tampa 18 Or 19 Days I Have Lost Count\n",
      "['raining', 'floding', 'florida', 'tampabay', 'tampa', 'days', 'lost', 'count']\n"
     ]
    }
   ],
   "source": [
    "#Option1 \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def common_stopwords(tweet):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "#     clean_mess = ' '.join(clean_mess)\n",
    "    return clean_mess\n",
    "\n",
    "print(train['text'].iloc[12])\n",
    "print(common_stopwords(train['text'].iloc[12]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option2 \n",
    "# We decided not to import the stopwords from nltk.corpus since we wanted to keep the words that negate like no,not,,\n",
    "\n",
    "# ### list of stop words that need to be removed\n",
    "# stop_words = ['as', 'in', 'of', 'is', 'are', 'were', 'was', 'it', 'for', 'to', 'from', 'into', 'onto', \n",
    "#               'this', 'that', 'being', 'the','those', 'these', 'such', 'a', 'an','i','and','be','you',\n",
    "#               'have','on','my','do','with', 'or','be','at','by','s','have']\n",
    "\n",
    "# from nltk import word_tokenize\n",
    "# import re\n",
    "\n",
    "# def remove_stopwords(tweet):\n",
    "#     tweet = re.sub('[^a-zA-Z]', ' ', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     tokenized_words = word_tokenize(tweet)\n",
    "#     temp = [word for word in tokenized_words if word not in stop_words]\n",
    "# #     temp = ' '.join(temp)\n",
    "#     return temp\n",
    "\n",
    "# print(train['text'].iloc[12])\n",
    "# print(remove_stopwords(train['text'].iloc[12]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between option A and B, we got a lower score using option B where we filter manual the stop words, so we will use option A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(common_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(common_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raining', 'floding', 'florida', 'tampabay', 'tampa', 'days', 'lost', 'count']\n",
      "['rain', 'floding', 'florida', 'tampabay', 'tampa', 'days', 'lose', 'count']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "print((train['text'].iloc[12]))   \n",
    "print(normalization((train['text'].iloc[12])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ce90958997c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fff' is not defined"
     ]
    }
   ],
   "source": [
    "fff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8- Removing repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatedletters(string):\n",
    "    postCheck = \"\"\n",
    "    previousChar = None\n",
    "    for currentChar in string:\n",
    "        if currentChar != previousChar:\n",
    "            postCheck += currentChar\n",
    "            previousChar = currentChar\n",
    "    return postCheck.title()\n",
    "\n",
    "print((train['text'].iloc[24]))   \n",
    "print(repeatedletters((train['text'].iloc[24])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(repeatedletters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(repeatedletters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Creating a new column with word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WordCount'] = train['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['WordCount'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(train)):\n",
    "#         train['text'][i] = ' '.join(train['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our dictionary \n",
    "uniqueWordFrequents = {}\n",
    "for tweet in train.text:\n",
    "    for word in tweet.split():\n",
    "        if(word in uniqueWordFrequents.keys()):\n",
    "            uniqueWordFrequents[word] += 1\n",
    "        else:\n",
    "            uniqueWordFrequents[word] = 1\n",
    "            \n",
    "#Convert dictionary to dataFrame\n",
    "uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
    "uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
    "uniqueWordFrequents.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
    "print(uniqueWordFrequents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
    "bagOfWords = counVec.fit_transform(train.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=uniqueWordFrequents.shape[0], min_df=5, max_df=0.7)  \n",
    "X = tfidfconverter.fit_transform(train.text).toarray()\n",
    "#if remove X, bagofwords should be X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# text_clf = Pipeline([('vect', CountVectorizer(max_features = uniqueWordFrequents.shape[0], min_df=5, max_df=0.7 )),\n",
    "#                       ('tfidf', TfidfTransformer()),\n",
    "#                       ('clf', MultinomialNB()) ])\n",
    "# text_clf = text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# predicted = text_clf.predict(X_test)\n",
    "# np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = text_clf.predict(X_test)\n",
    "# print(' F1 Score is      : ' ,f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = train.text\n",
    "y = train['target']\n",
    "print(\"X shape = \",X.shape)\n",
    "print(\"y shape = \",y.shape)\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20, random_state=55, shuffle =True)\n",
    "print('data splitting successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = None, \n",
    "                                           splitter='best', \n",
    "                                           random_state=55)\n",
    "\n",
    "decisionTreeModel.fit(X_train,y_train)\n",
    "\n",
    "print(\"decision Tree Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Train Score is   : ' ,decisionTreeModel.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "y_pred = decisionTreeModel.predict(X_test)\n",
    "print(' F1 Score is      : ' ,f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression = LogisticRegression(penalty='l2', \n",
    "                                        solver='saga', \n",
    "                                        random_state = 55)  \n",
    "\n",
    "LogisticRegression.fit(X_train,y_train)\n",
    "\n",
    "print(\"LogisticRegression Classifier model run successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Train Score is   : ' ,LogisticRegression.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LogisticRegression.predict(X_test)\n",
    "print(' F1 Score is      : ' ,f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('bow',CountVectorizer(analyzer=text_processing)),  # strings to token integer counts\n",
    "#     ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#     ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depending on accuracy choose option 1 or 2 in stop words and try to  remove not repetetive words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
